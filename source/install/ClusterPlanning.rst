Руководство по планированию кластера
=====================================

Руководство по планированию кластера включает в себя следующие главы необходимые для прочтения перед непосредственной установкой кластера: 

+ `Рекомендации по аппаратному обеспечению для Apache Hadoop`_
+ `Рекомендации по партиционированию файловой системы`_


Рекомендации по аппаратному обеспечению для Apache Hadoop
-----------------------------------------------------------

Рабочие нагрузки **Hadoop** и **HBase**, как правило, сильно различаются, и необходим опыт для правильного прогнозирования объемов хранилища, вычислительной мощности и межузловой связи, которые потребуются для выполнения различных видов работ.

В главе содержится информация о выборе соответствующих аппаратных компонентов для оптимального баланса между производительностью и начальными, а также текущими затратами:

+ `Типичный кластер Hadoop`_
+ `Шаблоны рабочей нагрузки для Hadoop`_
+ `Early Deployments`_
+ `Рекомендации по аппаратному обеспечению узла сервера`_

Краткое резюме приведено в разделе `Заключение`_.

**Hadoop** -- это программный фреймворк, поддерживающий крупномасштабный анализ распределенных данных на обычных серверах. **Arenadata** является участником инициатив с открытым исходным кодом (**Apache Hadoop**, **HDFS**, **Pig**, **Hive**, **HBase**, **ZooKeeper**) и имеет большой опыт управления кластерами **Hadoop** на производственном уровне. **Arenadata** рекомендует следовать принципам проектирования, которые обеспечивают масштабное развертывание в гиперпространстве. 

Для кластера **Hadoop** или **HBase** крайне важно точно спрогнозировать размер, тип, частоту и задержку для задач на выполнение. Начиная с **Hadoop** или **HBase**, есть возможность получения опыта, измеряя фактические рабочие нагрузки во время пилотного проекта. Таким образом, можно легко масштабировать пилотную среду, не внося существенных изменений в существующие серверы, программное обеспечение, стратегии развертывания и сетевое подключение.


Типичный кластер Hadoop
^^^^^^^^^^^^^^^^^^^^^^^^

Кластеры **Hadoop** и **HBase** имеют два типа машин:

+ Masters -- HDFS NameNode, YARN ResourceManager и HBase Master;
+ Slaves -- HDFS DataNodes, YARN NodeManagers и HBase RegionServers.

Узлы DataNodes, NodeManagers и HBase RegionServers размещаются и разворачиваются совместно для оптимальной локализации данных. Кроме того, **HBase** требует использования отдельного компонента (**ZooKeeper**) для управления кластером.

**Arenadata** рекомендует разделять master (основные) и slave (подчиненные) узлы, поскольку:

+ Рабочие нагрузки задачи/приложения на подчиненных узлах должны быть изолированы от основных;
+ Подчиненные узлы часто выводятся из эксплуатации для технического обслуживания.

В целях экспертизы можно развернуть **Hadoop**, используя инсталляцию с одним узлом (при условии, что все основные и подчиненные процессы находятся на одной машине). Для небольшого двухузлового кластера можно установить NameNode и ResourceManager на master-узле, а DataNode и NodeManager на slave-узле.

Кластеры из трех или более машин обычно используют один NameNode и ResourceManager со всеми другими узлами в качестве подчиненных. Кластер High-Availability использует первичный и вторичный NameNode, также как может использовать первичный и вторичный ResourceManager.

Как правило, кластер **Hadoop** среднего и большого размера состоит из двухуровневой или трехуровневой архитектуры, построенной на монтируемых в стойку серверах. Каждая стойка серверов соединена с помощью коммутатора *1 Gigabyte Ethernet* (GbE). Каждый коммутатор уровня стойки подключен к коммутатору уровня кластера (который, как правило, является коммутатором с большей плотностью портов *10GbE*). Коммутаторы кластерного уровня могут также соединяться с другими коммутаторами уровня кластера или даже восходить к другому уровню инфраструктуры.


Шаблоны рабочей нагрузки для Hadoop
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Дисковое пространство, пропускная способность ввода-вывода (необходимая для **Hadoop**) и вычислительная мощность (необходимая для процессов **MapReduce**) являются наиболее важными параметрами для точного определения размера оборудования. Кроме того, при установке **HBase** также необходимо проанализировать приложение и его требования к памяти, поскольку **HBase** является компонентом, интенсивно использующим память. Исходя из типичных вариантов использования **Hadoop**, в рабочих средах наблюдаются следующие шаблоны нагрузки:

+ **Balanced Workload** -- сбалансированная

Если рабочие нагрузки распределены равномерно между различными типами заданий (привязка к процессору, к дисковому вводу/выводу или к сетевому вводу/выводу), кластер имеет сбалансированный шаблон рабочей нагрузки. Это хорошая конфигурация по умолчанию для неизвестных или развивающихся нагрузок.

+ **Compute Intensive** -- ресурсоемкая

Рабочие нагрузки связаны с ЦП и характеризуются необходимостью большого количества процессоров и большого объема памяти для хранения данных в процессе. Шаблон использования типичен для обработки естественного языка или рабочих нагрузок HPCC.

+ **I/O Intensive** -- интенсивный ввод/вывод

Типичное задание MapReduce (например, сортировка) требует очень мало вычислительной мощности. Вместо этого оно больше зависит от связанной способности ввода/вывода кластера (например, при большом объеме холодных данных). Для такого типа рабочей нагрузки рекомендуется закладывать больше дисков в коробку.

+ **Unknown or evolving workload patterns** -- неизвестные или развивающиеся шаблоны рабочей нагрузки

Изначально можно не знать необходимые шаблоны рабочей нагрузки. И, как правило, первые задания, переданные в **Hadoop**, сильно отличаются от фактических, которые будут выполняться в производственной среде. По этим причинам **Arenadata** рекомендует либо использовать конфигурацию сбалансированной рабочей нагрузки, либо инвестировать в пилотный кластер **Hadoop** и планировать развитие его структуры по мере анализа шаблонов рабочей нагрузки в среде.


Early Deployments
^^^^^^^^^^^^^^^^^^^^

При первом знакомстве с **Hadoop** или **HBase** рекомендуется начинать с относительно небольшого экспериментального кластера, рассчитанного на Balanced Workload и получать опыт путем измерения фактических рабочих нагрузок во время пилотного проекта.

Для пилотного развертывания можно начать с 1U-машины и использовать следующие рекомендации:

+ Два четырехъядерных процессора;
+ От *12* до *24 ГБ* памяти;
+ От четырех до шести дисков емкостью *2 ТБ*.

Минимальное требование для сети составляет *1GigE* all-to-all и может быть легко достигнуто путем подключения всех узлов к коммутатору Gigabyte Ethernet. Чтобы использовать запасной сокет для добавления процессоров в будущем, можно использовать шести или восьми ядерный процессор.

Для небольших и средних кластеров **HBase** необходимо предоставить каждому серверу **ZooKeeper** около *1 ГБ* RAM и желательно собственный диск.


Jump-start -- Hadoop Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Один из способов быстрого развертывания **Hadoop** -- выбрать "cloud trials" или использовать виртуальную инфраструктуру. **Arenadata** обеспечивает доступность дистрибутива через платформу данных **Enterprise Data Platform** (**EDP**), которую можно установить в общедоступных и частных облаках с помощью **Whirr**, **Microsoft Azure** и **Amazon Web Services**.

Обратиться в службу технической поддержки **Arenadata** можно по адресу электронной почты info@arenadata.io или через окно консультации на сайте `www.arenadata.io <https://arenadata.tech/>`_. 

Однако, облачные сервисы и виртуальные инфраструктуры не предназначены для **Hadoop**. В этом случае развертывания **Hadoop** и **HBase** могут иметь низкую производительность из-за виртуализации и неоптимальной архитектуры ввода/вывода.


Контроль ресурсов при пилотном развертывании
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Arenadata** рекомендует контролировать пилотный кластер с помощью **Ganglia**, **Nagios** или других систем мониторинга производительности. При этом важно:

+ Измерить использование ресурсов для CPU, RAM, операций дискового ввода/вывода в секунду (IOPS) и отправленных и полученных сетевых пакетов. Запустить актуальные виды запросов или аналитических заданий;

+ Убедиться, что подмножество данных масштабируется до размера пилотного кластера;

+ Проанализировать данные мониторинга на предмет насыщения ресурсов. На основе этого анализа можно классифицировать задания как связанные с процессором, с дисковым вводом/выводом или с сетевым вводом/выводом.

.. important:: Большинство приложений Java расширяют использование RAM до максимально допустимого. Однако такие задания не следует анализировать как связанные с памятью, если только не происходит подкачка или JVM garbage collection (узел прекращает всю полезную работу на несколько минут)

+ Проанализировать **ZooKeeper** (так как в нем часто обнаруживаются проблемы, свзязанные с сетью и памятью для **HBase**).

+ (Опционально) Настроить параметры работы и конфигурации оборудования или сети, чтобы сбалансировать использование ресурсов. Если задания попадают в разные шаблоны рабочей нагрузки, можно выбрать управление только параметрами задания, а для оборудования оставить "balanced".


Challenges -- настройка job-характеристик на использование ресурсов
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Способ кодирования задания или представления данных может оказать большое влияние на баланс ресурсов. Например, затраты ресурсов могут быть смещены между дисковым IOPS и CPU с учетом выбранной схемы сжатия или формата синтаксического анализа. Процессор для каждого узла и активность диска можно обменять на пропускную способность между узлами в зависимости от реализации стратегии Map/Reduce.

Кроме того, Amdahl’s Law показывает, как требования к ресурсам могут меняться в значительной степени нелинейным образом при изменении требований: изменение, которое, как можно ожидать, приведет к снижению затрат на вычисления на 50%, может вместо этого привести к изменению чистой производительности на 10% или на 90%.


Повторное использование пилотных машин
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Установив пилотный кластер, можно приступить к анализу шаблонов рабочих нагрузок для выявления узких мест CPU и I/O. Позже эти машины могут быть повторно использованы в производственных кластерах, даже если базовые характеристики изменятся. 

Чтобы добиться положительного коэффицента возврата инвестиций (return on investment, ROI), рекомендуется убедиться, что машины в пилотных кластерах составляют менее *10%* от конечного производственного кластера.


Рекомендации по аппаратному обеспечению узла сервера
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

В разделе приведены рекомендации по аппаратному обеспечению узла сервера для выбора количества узлов, параметров хранилища на узел (количество дисков и их размер, MTBF и затраты на репликацию при сбоях дисков), вычислительной мощности на узел (сокеты, ядра, тактовая частота), RAM на узел и возможности сети (количество, скорость портов).

Slave узлы
~~~~~~~~~~~

**Серверная платформа**

Как правило, серверы с двумя сокетами являются оптимальными для развертывания **Hadoop**. Для средних и больших кластеров их использование является лучшим выбором по сравнению с серверами начального уровня благодаря возможности балансировки нагрузки и распараллеливания. С точки зрения компактности выбирается серверное оборудование, которое подходит для небольшого количества стоек. Обычно серверы *1U* или *2U* используются в *19"* стойках или шкафах.

**Возможности хранения**

Для приложений общего назначения рекомендуется использовать относительно большое количество жестких дисков, обычно от *8* до *12* дисков SATA LFF на сервер. В настоящее время типичная емкость **Hadoop** в производственных средах составляет около *2 ТБ* на диск. Высокоинтенсивные среды ввода/вывода могут потребовать диски *12x2 TБ* SATA. Оптимальный баланс между затратами и производительностью, как правило, достигается с помощью дисков SATA емкостью *7200 об/мин*. Если в хранилище прогнозируется значительный рост, следует рассмотреть возможность использования дисков *3 ТБ*.

SFF-диски используются в некоторых конфигурациях для лучшей пропускной способности диска. Рекомендуется отслеживать кластер на предмет возможных сбоев дисков, так как повышение их количества приводит и к увеличению частоты сбоев. Если количество дисков на сервере велико, следует использовать два дисковых контроллера, чтобы нагрузка ввода/вывода могла распределяться между несколькими ядрами. Настоятельно рекомендуется использовать только SATA или SAS.

В **HDFS**, в котором используется недорогая надежная система хранения, данные остаются неограниченное время и потребности в хранилище быстро растут. С 12-дисковыми системами обычно получается *24* или *36 ТБ* на узел. Использование такой емкости хранилища в узле целесообразно только с версией **Hadoop 1.0.0** и выше.

**Hadoop** -- это интенсивное и эффективное хранилище, не требующее при этом быстрых и дорогих жестких дисков. Если шаблон рабочей нагрузки не является I/O Intensive, можно добавить только четыре или шесть дисков на узел. Важно понимать, что затраты на электроэнергию пропорциональны количеству дисков, а не емкости хранилища. 

.. important:: RAID vs. JBOD: не рекомендуется использовать RAID на slave-машинах Hadoop. Кластер допускает вероятность сбоя диска и обеспечивает избыточность данных на всех подчиненных узлах

.. important:: Диски должны иметь хорошие значения MTBF, так как подчиненные узлы в Hadoop подвержены сбоям

Подчиненные узлы не нуждаются в дорогостоящей поддержке, предлагающей услуги замены дисков в течение двух часов или меньше. **Hadoop** адаптирован к отказам slave-узлов, и поэтому следует относиться к работам по обслуживанию подчиненных узлов как к постоянной задаче, а не как к чрезвычайной ситуации.

Хорошо иметь возможность замены дисков, без изъятия сервера из стойки, хотя при этом отключение стойки (непродолжительное) является недорогой операцией в кластере.

**Размер памяти**

Крайне важно обеспечить достаточную память, чтобы процессоры были заняты без подкачки и без чрезмерных затрат на нестандартные материнские платы. В зависимости от количества ядер подчиненным узлам обычно требуется от *24* до *48 ГБ* оперативной памяти. Для больших кластеров этот объем памяти обеспечивает достаточно дополнительной RAM (приблизительно *4 ГБ*) для платформы **Hadoop** и для процессов запросов и анализа (**HBase** и/или Map/Reduce).

Для обнаружения и исправления случайных нерегулярных ошибок настоятельно рекомендуется использовать память с error correcting code (ECC). Исправление ошибок RAM позволяет доверять качеству вычислений. `Было доказано <http://www.cs.utoronto.ca/~bianca/papers/sigmetrics09.pdf>`_, что некоторые детали (chip-kill/chip spare) обеспечивают лучшую защиту с меньшим повторением битовых ошибок, чем традиционные конструкции.

При желании сохранения возможности добавления дополнительной памяти на серверы в будущем необходимо убедиться, что для этого есть место рядом с начальными модулями памяти.

**Подготовка памяти**

Память может представлять собой недорогие материнские платы на серверах низкого уровня, что типично для технологии Over-Provisioning. Неиспользуемая RAM в таком случае применяется либо приложениями **Hadoop** (обычно при параллельном запуске нескольких процессов), либо инфраструктурой (для кэширования данных на диске с целью повышения производительности).

**Процессоры**

Хотя важно понимать шаблон рабочей нагрузки, рекомендуется использовать процессоры со средней тактовой частотой и менее, чем с двумя сокетами. Для большинства рабочих нагрузок дополнительная производительность на узел не является экономически эффективной. Следует использовать как минимум два четырехъядерных процессора для подчиненных машин больших кластеров.

**Power considerations**

Мощность является главным беспокойством при проектировании кластеров **Hadoop**. Вместо того, чтобы приобретать самые большие и быстрые узлы, советуется проанализировать расход энергии для имеющегося оборудования. Существует огромная экономия в ценах и энергопотреблении путем избежания самых быстрых процессоров, резервных источников питания и прочего.

Производители создают машины для облачных центров обработки данных с целью снижения затрат и энергопотребления и являющиеся при этом легковесными. Например, **Supermicro**, **Dell** и **HP** имеют такие линейки продуктов для облачных провайдеров. Поэтому при необходимости закупок в большом объеме, рекомендуется оценить эти упрощенные "cloud servers".






Заключение
^^^^^^^^^^^


Рекомендации по партиционированию файловой системы
---------------------------------------------------
