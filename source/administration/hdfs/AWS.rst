Модуль Hadoop-AWS: интеграция с Amazon Web Services
====================================================

.. important:: В Hadoop коннекторы s3: и s3n: удалены. В качестве коннектора для данных, размещенных в S3 с Apache Hadoop, используется s3a:

Как перейти на клиент **S3A**:

1. Сохранить hadoop-aws JAR в classpath.

2. Добавить JAR-бандл aws-java-sdk-bundle.jar, который поставляется с Hadoop, в classpath.

3. Изменить ключи аутентификации:

+ ``fs.s3n.awsAccessKeyId`` --> ``fs.s3a.access.key``;

+ ``fs.s3n.awsSecretAccessKey`` --> ``fs.s3a.secret.key``;

Важно убедиться, что имена свойств верны. Для **S3A** это *fs.s3a.access.key* и *fs.s3a.secret.key* -- нельзя просто скопировать свойства **S3N** и заменить *s3n* на *s3a*.

4. Заменить все URL, которые начинаются с ``s3n://`` на ``s3a://``.

5. Удалить jets3t JAR, так как он больше не нужен.

Модуль **Apache Hadoop** -- **hadoop-aws**, обеспечивает поддержку интеграции с **AWS** (**Amazon Web Services**). 

Для включения клиента **S3A** в classpath **Apache Hadoop** по умолчанию необходимо:

1. Убедиться, что ``HADOOP_OPTIONAL_TOOLS`` в *hadoop-env.sh* включает hadoop-aws в свой список дополнительных модулей для добавления в classpath.

2. Для взаимодействия на стороне клиента можно объявить, что соответствующие JAR-файлы должны быть загружены в файл *~/.hadooprc*:

::

 hadoop_add_to_classpath_tools hadoop-aws

Параметры в этом файле не распространяются на развернутые приложения, но работают для локальных клиентов, таких как команда ``hadoop fs``.

Клиент **S3A** предлагает высокопроизводительный ввод-вывод по сравнению с хранилищем объектов **Amazon S3** и совместимыми реализациями:

+ Непосредственно читает и пишет S3-объекты;

+ Совместим со стандартными S3-клиентами;

+ Совместим с файлами, созданными более старым клиентом *s3n://* и клиентом Amazon EMR *s3://*;

+ Поддерживает партиционированную загрузку для объектов размером в несколько ГБ;

+ Предлагает высокопроизводительный режим случайного ввода-вывода для работы со столбчатыми данными, такими как файлы Apache ORC и Apache Parquet;

+ Использует Java S3 SDK от Amazon с поддержкой новейших функций S3 и схем аутентификации;

+ Поддерживает аутентификацию с помощью переменных среды, свойств конфигурации Hadoop, хранилища ключей Hadoop и ролей IAM;

+ Поддерживает конфигурацию для каждого сегмента;

+ С помощью S3Guard добавляет высокопроизводительные и согласованные операции чтения метаданных/каталогов, что обеспечивает последовательность и скорость;

+ Поддерживает S3 "Server Side Encryption" для чтения и записи: SSE-S3, SSE-KMS и SSE-C;

+ Инструментирован с метриками Hadoop;

+ Активно поддерживается сообществом открытого исходного кода.



Getting Started
Warnings
Authenticating with S3
Protecting the AWS Credentials
Storing secrets with Hadoop Credential Providers
General S3A Client configuration
Retry and Recovery
Configuring different S3 buckets with Per-Bucket Configuration
How S3A writes data to S3
Metrics
Other Topics
