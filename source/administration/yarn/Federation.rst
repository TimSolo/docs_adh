Hadoop: YARN Federation
========================

Архитектура
-------------

Известно, что **YARN** масштабируется до тысяч узлов. Масштабируемость **YARN** определяется **Resource Manager**, и она пропорциональна количеству узлов, активных приложений и контейнеров, а так же частоте heartbeat-сообщений (как узлов, так и приложений). Снижение heartbeat-сообщений может обеспечить увеличение масштабируемости, однако это отрицательно сказывается на использовании. В данной главе описан подход на основе Federation для масштабирования одного кластера **YARN** до десятков тысяч узлов путем интеграции нескольких подкластеров **YARN**. Предлагаемый метод заключается в разделении большого кластера (10-100 тысяч узлов) на более мелкие блоки, называемые субкластерами (sub-cluster), каждый из которых имеет свой собственный **YARN Resource Manager** и вычислительные узлы. Система Federation объединяет эти субкластеры и делает их одним большим кластером **YARN** для приложений. Приложения при этом видят один массивный кластер **YARN** и могут планировать задачи на любом его узле, в то время как в рамках системы Federation ведутся переговоры с **Resource Manager** субкластеров для предоставления ресурсов приложению. Цель состоит в том, чтобы позволить отдельной задаче бесшовно "охватить" субкластеры.

Такая конструкция является структурно масштабируемой, поскольку она связывает количество узлов, за которые отвечает каждый **Resource Manager**, а соответствующие политики пытаются обеспечить, чтобы большинство приложений находилось в одном субкластере, таким образом, число видимых приложений для каждого **Resource Manager** также ограничено. Это означает, что масштабирумость может быть почти линейной, просто добавляя субкластеры (поскольку для них требуется очень небольшая координация). Такая архитектура может обеспечить очень строгое соблюдение инвариантов планирования в каждом субкластере (просто наследуется от **YARN**), в то время как непрерывная перебалансировка по субкластеру обеспечивает (менее строго) то, что эти свойства также соблюдаются на глобальном уровне (например, если субкластер теряет большое количество узлов, можно переназначить очереди на другие субкластеры, чтобы обеспечить исключение несправедливого воздействия на пользователей, работающих в поврежденном субкластере).

Federation спроектирована как "слой" поверх существующей кодовой базы **YARN** с ограниченными изменениями в основных механизмах (:numref:`Рис.%s.<federation_architecture>`).

.. _federation_architecture:

.. figure:: ../../imgs/administration/yarn/federation_architecture.png
   :align: center

   Основные компоненты, составляющие кластер Federation



YARN Sub-cluster
^^^^^^^^^^^^^^^^^^

Субкластер (sub-cluster) -- это кластер **YARN** размером до нескольких тысяч узлов. Точный размер субкластера определяется с учетом простоты развертывания/обслуживания, согласования с сетевыми зонами и их доступности, а также общими рекомендациями.

**Resource Manager** субкластера **YARN** работает с высоким уровнем доступности с сохранением работоспособности, то есть необходимо быть в состоянии к сбоям **Resource Manager** и  **Node Manager** с минимальными нарушениями. Если весь субкластер скомпрометирован, внешние механизмы обеспечат повторную передачу заданий в отдельный субкластер (в дальнейшем это может быть включено в Federation).

Субкластер также является единицей масштабируемости в среде Federation -- ее можно расширить, добавив один или несколько субкластеров.

По своей структуре каждый субкластер является полностью функциональным **Resource Manager**, и его вклад в Federation может быть установлен лишь на долю его общей емкости, т.е. субкластер может иметь "частичное" обязательство перед Federation, сохраняя при этом способность выдавать часть своих возможностей локальным способом.


Router
^^^^^^^

Приложения **YARN** отправляются на один из маршрутизаторов (Router), который, в свою очередь, применяет политику маршрутизации (полученную из Policy Store), запрашивает в State Store URL-адрес субкластера и перенаправляет запрос на отправку приложения в соответствующий **Resource Manager** субкластера. Субкластер, в котором запускается задание, называется "домашним субкластером" ("home sub-cluster"), а "вторичными субкластерами" ("secondary sub-clusters") называются все остальные субкластеры, на которые распространяется задание. 

Маршрутизатор предоставляет *ApplicationClientProtocol* внешнему миру, прозрачно скрывая присутствие нескольких **Resource Manager**. Для этого маршрутизатор также сохраняет соответствие между приложением и его домашним субкластером в State Store. Это позволяет маршрутизаторам быть в мягком состоянии, недорого поддерживая при этом запросы пользователей, так как любой маршрутизатор может восстановить приложение для маппинга домашнего субкластера и направить запросы к нужному **Resource Manager**. Целесообразно для кэширования производительности и балансировки нагрузки. Состояние Federation (включая приложения и узлы) отображается через веб-интерфейс.


AMRMProxy
^^^^^^^^^^^

*AMRMProxy* является ключевым компонентом, позволяющим приложению масштабироваться и работать в субкластерах. *AMRMProxy* работает на всех машинах **Node Manager** и действует как прокси-сервер для **YARN Resource Manager** для **Application Master**, реализуя *ApplicationMasterProtocol*. Приложениям не разрешается напрямую связываться с **Resource Manager** субкластера. Система принудительно подключает их только к конечной точке *AMRMProxy*, что обеспечивает прозрачный доступ к нескольким **YARN Resource Manager** (путем динамической маршрутизации / разделения / маппинга коммуникаций). В любой момент времени задание может охватывать один домашний и несколько вторичных субкластеров, но работающие в *AMRMProxy* политики пытаются ограничить площадь каждого задания, чтобы минимизировать накладные расходы на инфраструктуру планирования (:numref:`Рис.%s.<amrmproxy_architecture>`).

.. _amrmproxy_architecture:

.. figure:: ../../imgs/administration/yarn/amrmproxy_architecture.png
   :align: center

   Архитектура цепочки перехватчиков AMRMProxy


Роль *AMRMProxy*:

+ Защита субкластера YARN Resource Manager от некорректно работающих Application Master. AMRMProxy может предотвратить DDOS-атаки, дросселируя/уничтожая требующих слишком много ресурсов Application Master;

+ Маскировка нескольких YARN Resource Manager в кластере и прозрачный допуск Application Master к распределению по субкластерам. Все распределения контейнеров выполняются инфраструктурой YARN Resource Manager, которая состоит из AMRMProxy, выходящего в домашний и другие субкластера Resource Manager;





Global Policy Generator
^^^^^^^^^^^^^^^^^^^^^^^^


Federation State-Store
^^^^^^^^^^^^^^^^^^^^^^^^


**Sub-cluster Membership**

**Application’s Home Sub-cluster**


Federation Policy Store
^^^^^^^^^^^^^^^^^^^^^^^^


Running Applications across Sub-Clusters
------------------------------------------


Configuration
---------------


EVERYWHERE:
^^^^^^^^^^^^

**State-Store:**


**Optional:**


ON RMs:
^^^^^^^^


ON ROUTER:
^^^^^^^^^^^


ON NMs:
^^^^^^^^


Running a Sample Job
----------------------

