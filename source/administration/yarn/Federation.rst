Hadoop: YARN Federation
========================

Архитектура
-------------

Известно, что **YARN** масштабируется до тысяч узлов. Масштабируемость **YARN** определяется **Resource Manager**, и она пропорциональна количеству узлов, активных приложений и контейнеров, а так же частоте heartbeat-сообщений (как узлов, так и приложений). Снижение heartbeat-сообщений может обеспечить увеличение масштабируемости, однако это отрицательно сказывается на использовании. В данной главе описан подход на основе Federation для масштабирования одного кластера **YARN** до десятков тысяч узлов путем интеграции нескольких подкластеров **YARN**. Предлагаемый метод заключается в разделении большого кластера (10-100 тысяч узлов) на более мелкие блоки, называемые субкластерами (sub-cluster), каждый из которых имеет свой собственный **YARN Resource Manager** и вычислительные узлы. Система Federation объединяет эти субкластеры и делает их одним большим кластером **YARN** для приложений. Приложения при этом видят один массивный кластер **YARN** и могут планировать задачи на любом его узле, в то время как в рамках системы Federation ведутся переговоры с **Resource Manager** субкластеров для предоставления ресурсов приложению. Цель состоит в том, чтобы позволить отдельной задаче бесшовно "охватить" субкластеры.

Такая конструкция является структурно масштабируемой, поскольку она связывает количество узлов, за которые отвечает каждый **Resource Manager**, а соответствующие политики пытаются обеспечить, чтобы большинство приложений находилось в одном субкластере, таким образом, число видимых приложений для каждого **Resource Manager** также ограничено. Это означает, что масштабирумость может быть почти линейной, просто добавляя субкластеры (поскольку для них требуется очень небольшая координация). Такая архитектура может обеспечить очень строгое соблюдение инвариантов планирования в каждом субкластере (просто наследуется от **YARN**), в то время как непрерывная перебалансировка по субкластеру обеспечивает (менее строго) то, что эти свойства также соблюдаются на глобальном уровне (например, если субкластер теряет большое количество узлов, можно переназначить очереди на другие субкластеры, чтобы обеспечить исключение несправедливого воздействия на пользователей, работающих в поврежденном субкластере).

Federation спроектирована как "слой" поверх существующей кодовой базы **YARN** с ограниченными изменениями в основных механизмах (:numref:`Рис.%s.<federation_architecture>`).

.. _federation_architecture:

.. figure:: ../../imgs/administration/yarn/federation_architecture.png
   :align: center

   Основные компоненты, составляющие кластер Federation



YARN Sub-cluster
^^^^^^^^^^^^^^^^^^

Субкластер (sub-cluster) -- это кластер **YARN** размером до нескольких тысяч узлов. Точный размер субкластера определяется с учетом простоты развертывания/обслуживания, согласования с сетевыми зонами и их доступности, а также общими рекомендациями.

**Resource Manager** субкластера **YARN** работает с высоким уровнем доступности с сохранением работоспособности, то есть необходимо быть в состоянии терпеть сбои **Resource Manager** и  **NodeManager** с минимальными нарушениями. Если весь субкластер скомпрометирован, внешние механизмы обеспечат повторную передачу заданий в отдельный субкластер (в дальнейшем это может быть включено в Federation).

Субкластер также является единицей масштабируемости в среде Federation -- ее можно расширить, добавив один или несколько субкластеров.

По своей структуре каждый субкластер является полностью функциональным Resource Manager, и его вклад в Federation может быть установлен лишь на долю его общей емкости, т.е. субкластер может иметь "частичное" обязательство перед Federation, сохраняя при этом способность выдавать часть своих возможностей локальным способом.


Router
^^^^^^^


AMRMProxy
^^^^^^^^^^^


Global Policy Generator
^^^^^^^^^^^^^^^^^^^^^^^^


Federation State-Store
^^^^^^^^^^^^^^^^^^^^^^^^


**Sub-cluster Membership**

**Application’s Home Sub-cluster**


Federation Policy Store
^^^^^^^^^^^^^^^^^^^^^^^^


Running Applications across Sub-Clusters
------------------------------------------


Configuration
---------------


EVERYWHERE:
^^^^^^^^^^^^

**State-Store:**


**Optional:**


ON RMs:
^^^^^^^^


ON ROUTER:
^^^^^^^^^^^


ON NMs:
^^^^^^^^


Running a Sample Job
----------------------

