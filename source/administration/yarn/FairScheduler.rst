Hadoop: Fair Scheduler
=======================

В главе описывается **FairScheduler** -- подключаемый планировщик для **Hadoop**, позволяющий YARN-приложениям справедливо распределять ресурсы в больших кластерах.

Справедливое планирование -- это метод распределения ресурсов между приложениями таким образом, чтобы все приложения в среднем получали равную долю ресурсов с течением времени. **Hadoop NextGen** способен планировать несколько типов ресурсов. По умолчанию в **FairScheduler** планирование решений основывается только на памяти. Но он также может быть сконфигурирован для планирования, основанного на памяти вместе с процессором, используя понятие Dominant Resource Fairness, разработанное **Ghodsi et al**. 

Когда запущено одно приложение, оно использует весь кластер. А при добавлении новых приложений, им назначаются освобождающиеся ресурсы, таким образом каждое приложение в конечном итоге получает примерно одинаковый объем ресурсов. В отличие от стандартного планировщика **Hadoop**, который формирует очередь приложений, **FairScheduler** позволяет коротким приложениям завершать работу в разумные сроки, не оставляя ждать при этом долговременные приложения. Это также разумный способ разделить кластер между несколькими пользователями. Наконец, справедливое распределение может также работать с приоритетностью приложений -- приоритеты используются в качестве веса для определения доли ресурсов от их совокупности, которую должны получить приложения.

Далее планировщик организует приложения в "очереди" и справедливо распределяет ресурсы между этими очередями. По умолчанию все пользователи имеют общую очередь с именем *default*. Если приложение специально указывает очередь в запросе ресурса контейнера, запрос отправляется в эту очередь. Также можно назначать очереди на основе имени пользователя, включенного в запрос, через конфигурацию. В каждой очереди используется политика планирования для совместного использования ресурсов между запущенными приложениями. По умолчанию используется распределение ресурсов на основе памяти, но также могут быть настроены *FIFO* и мультиресурсность с Dominant Resource Fairness. Очереди могут быть организованы в иерархию для разделения ресурсов и настроены с весами для совместного использования кластера в определенных пропорциях.

Кроме этого, **FairScheduler** позволяет назначать гарантированные минимальные доли в очереди, что полезно для обеспечения определенных пользователей, групп или рабочих приложений достаточными ресурсами. Когда очередь содержит приложения, она получает по крайней мере свою минимальную долю, но когда очередь не нуждается в полной гарантированной доле, избыток распределяется между другими запущенными приложениями. Это позволяет планировщику гарантировать пропускную способность очередей при эффективном использовании ресурсов, когда эти очереди не содержат приложений.

**FairScheduler** позволяет запускать все приложения по умолчанию, но также через файл конфигурации можно ограничить количество запущенных приложений на пользователя и на очередь. Это может быть полезно, когда пользователь должен одновременно отправить сотни приложений, или в целом для повышения производительности, если запуск слишком большого количества приложений может привести к созданию слишком большого объема промежуточных данных или слишком частому переключений контекста. Ограничение приложений не приводит к сбою каких-либо последующих отправленных приложений, а только к ожиданию в очереди планировщика, пока не завершатся некоторые из более ранних приложений пользователя.


Иерархия очередей и политика
------------------------------

**FairScheduler** поддерживает иерархию очередей. Все очереди происходят из очереди с именем *root*. Доступные ресурсы распределяются между дочерними элементами root-очереди типичным способом справедливого планирования. Затем дочерние очереди таким же образом распределяют выделенные им ресурсы по своим дочерним очередям. Приложения могут быть запланированы только в leaf-очередях. Очереди можно указывать как дочерние элементы других очередей, помещая их как подэлементы в файл распределения.

Имя очереди начинается с перечисления имен ее родителей с точками в качестве разделителей. Таким образом, очередь с именем "queue1" в root-очереди называется "root.queue1", а очередь с именем "queue2" в очереди с именем "parent1" называется "root.parent1.queue2". При обращении к очередям часть имени *root* необязательна, поэтому *queue1* может называться просто "queue1", а *queue2* -- "parent1.queue2".

Кроме того, **FairScheduler** позволяет устанавливать различные индивидуальные политики для каждой очереди, чтобы разрешить совместное использование ресурсов любым удобным для пользователя способом. Политика может быть построена путем расширения ``org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy``. *FifoPolicy*, *FairSharePolicy* (по умолчанию) и *DominantResourceFairnessPolicy* являются встроенными и могут быть легко использованы.

Некоторые дополнения еще не поддерживаются в оригинальном (**MR1**) **Fair Scheduler**. Среди них -- использование индивидуальных политик, управляющих приоритетом "boosting" над определенными приложениями.


Размещение приложений в очередях
----------------------------------

Планировщик **Fair Scheduler** позволяет администраторам настраивать политики, которые автоматически помещают отправленные приложения в соответствующие очереди. Размещение может зависеть от пользователя и групп отправителя и запрашиваемой очереди. Политика состоит из набора правил, которые применяются последовательно для классификации входящего приложения. Каждое правило либо помещает приложение в очередь, либо отклоняет его, либо переходит к следующему правилу. Далее в документации приведен `Формат файла распределения`_ для настройки этих политик.


Установка
-------------

Для использования **Fair Scheduler** сначала необходимо назначить соответствующий класс планировщика в *yarn-site.xml*:

::

 <property>
   <name>yarn.resourcemanager.scheduler.class</name>
   <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
 </property>


Configuration
----------------

Настройка планировщика **Fair Scheduler** обычно включает в себя изменение двух файлов. Во-первых, можно настроить параметры всего планировщика, добавив свойства конфигурации в файл *yarn-site.xml* в существующую директорию. Во-вторых, в большинстве случаев пользователи желают создать список файлов распределения, в котором указываются существующие очереди и соответствующий им вес и пропускная способность. Файл распределения перезагружается каждые *10* секунд, позволяя вносить изменения на лету.

Свойства в файле yarn-site.xml 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``yarn.scheduler.fair.allocation.file`` -- путь к файлу распределения. Файл распределения -- это XML-манифест, описывающий очереди и их свойства в дополнение к определенным параметрам политики по умолчанию. Этот файл должен быть в формате XML, описанном в следующем параграфе. Если указан относительный путь, то поиск файла осуществляется по classpath (который обычно включает в себя каталог *Hadoop conf*). По умолчанию используется *fair-scheduler.xml*.

``yarn.scheduler.fair.user-as-default-queue`` -- определяет, следует ли использовать имя пользователя, связанное с распределением, в качестве имени очереди по умолчанию, если другого не указано. Если для параметра установлено значение *false* или не задано вовсе, все задачи имеют общую очередь по умолчанию с именем "default". По умолчанию значение параметра устанавливается на *true*. Если в файле распределения указывается политика размещения в очереди, то данное свойство игнорируется.

``yarn.scheduler.fair.preemption`` -- определяет, следует ли использовать преимущественное право preemption. По умолчанию устанавливается на *false*.

``yarn.scheduler.fair.preemption.cluster-utilization-threshold`` -- порог использования, после которого вступает в действие преимущественное право preemption. Вычисляется как максимальное отношение использования к пропускной способности среди всех ресурсов. По умолчанию задается *0,8f*.

``yarn.scheduler.fair.sizebasedweight`` -- определяет, следует ли назначать общие ресурсы отдельным приложениям, основываясь на их размере, вместо того, чтобы предоставлять равные ресурсы всем приложениям независимо от их размера. При значении *true* приложения взвешиваются по натуральному логарифму -- единица плюс вся запрашиваемая память приложения, поделенная на натуральный логарифм *2*. По умолчанию значение *false*.

``yarn.scheduler.fair.assignmultiple`` -- определяет, разрешить ли назначение нескольких контейнеров в одном heartbeat-сообщении. По умолчанию *false*.

``yarn.scheduler.fair.dynamic.max.assign`` -- устанавливает, следует ли динамически определять количество ресурсов, которое может быть назначено за одно heartbeat-сообщение, если для атрибута *assignmultiple* задано значение *true*. При включенном параметре около половины нераспределенных ресурсов на узле распределяются по контейнерам за одинарное heartbeat-сообщение. По умолчанию *true*.

``yarn.scheduler.fair.max.assign`` -- максимальное количество контейнеров, которое может быть назначено за одно heartbeat-сообщение, при условии: значение *assignmultiple* задано *true*, а для *dynamic.max.assign* равно *false*. По умолчанию параметр устанавливается в *-1*, что не задает никаких ограничений.

``yarn.scheduler.fair.locality.threshold.node`` -- число возможностей планирования для приложений, которые запрашивают контейнеры на определенных узлах, с момента последнего назначения контейнера в ожидании перед принятием размещения на другом узле. Выражается в виде числа с плавающей запятой (float) от *0* до *1*, которое в виде доли от размера кластера представляет собой количество возможностей планирования, которые необходимо упустить. Значение по умолчанию *-1.0* означает, что никаких возможностей планирования упускаться не будет.

``yarn.scheduler.fair.locality.threshold.rack`` -- 

``yarn.scheduler.fair.allow-undeclared-pools`` -- 

``yarn.scheduler.fair.update-interval-ms`` -- 

``yarn.resource-types.memory-mb.increment-allocation`` -- 

``yarn.resource-types.vcores.increment-allocation`` -- 

``yarn.resource-types.<resource>.increment-allocation`` -- 

``yarn.scheduler.increment-allocation-mb`` -- 

``yarn.scheduler.increment-allocation-vcores`` -- 



Формат файла распределения
^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Queue Access Control Lists
^^^^^^^^^^^^^^^^^^^^^^^^^^^


Reservation Access Control Lists
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Configuring ReservationSystem
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Administration
----------------

Modifying configuration at runtime
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Monitoring through web UI
^^^^^^^^^^^^^^^^^^^^^^^^^^


Moving applications between queues
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


Dumping Fair Scheduler state
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


