Hadoop: Capacity Scheduler
============================

В главе описывается **CapacityScheduler** -- подключаемый планировщик для **Hadoop**, позволяющий при мультитенантности безопасно совместно использовать большой кластер таким образом, чтобы для приложений своевременно распределялись ресурсы в условиях ограниченно выделенных мощностей.

**CapacityScheduler** предназначен для запуска приложений **Hadoop** в виде общего мультитенантного кластера удобным для оператора способом при максимальной пропускной способности и загрузке кластера.

Традиционно каждая организация имеет свой собственный набор вычислительных ресурсов, которые имеют достаточную производительность для соответствия SLA предприятия в пиковых или около пиковых условиях. Как правило, это приводит к низкой средней загрузке и накладным расходам на управление несколькими независимыми кластерами по одному на каждую организацию. Поэтому совместное использование кластеров между несколькими организациями -- это рентабельный способ запуска крупных Hadoop-инсталляций, так как это позволяет пользоваться преимуществами  масштаба, не создавая частных кластеров. Однако организации обеспокоены совместным использованием кластера в вопросе использования другими предприятиями ресурсов, критически важных для их собственного SLA.

**CapacityScheduler** предназначен для совместного использования большого кластера, предоставляя при этом каждой организации гарантии производительности. Основная идея заключается в том, что доступные ресурсы в кластере **Hadoop** распределяются между несколькими предприятиями. Дополнительным преимуществом является то, что организация может получить доступ к любой избыточной мощности, не используемой другими. Это обеспечивает гибкость экономически эффективным образом.

Совместное использование кластеров требует строгой мультитенантности, поскольку каждому предприятию должна быть обеспечена производительность и безопасность, чтобы гарантировать, что общий кластер защищен от любого постороннего приложения или пользователя. **CapacityScheduler** предоставляет обязательный набор ограничений, гарантирующих, что отдельное приложение, пользователь или очередь не могут использовать непропорционально большое количество ресурсов в кластере. Кроме того, для обеспечения справедливости и стабильности кластера планировщик предоставляет для инициализированных и ожидающих приложений от одного пользователя очереди и ограничения.

Основной абстракцией, предоставляемой **CapacityScheduler**, является концепция очередей. Они обычно настраиваются администраторами и отражают экономику общего кластера.

С целью дополнительного контроля и предсказуемости при совместном использовании ресурсов **CapacityScheduler** также поддерживает иерархические очереди, чтобы обеспечить распределение ресурсов между под-очередями среди приложений внутри одной организации, прежде чем другим очередям будет позволено использовать свободные ресурсы.


Функции
-----------

**CapacityScheduler** поддерживает следующие функции:

+ Иерархические очереди (Hierarchical Queues). 

Поддерживается иерархия очередей, обеспечивающая совместное использование ресурсов между под-очередями внутри организации, прежде чем другим очередям будет позволено использовать свободные ресурсы, что обеспечивает больший контроль и предсказуемость.

+ Гарантии поизводительности (Capacity Guarantees). 

Очереди распределяются по части пропускной способности сети в том смысле, что в их распоряжении находится определенная производительность ресурсов. Все приложения, отправленные в очередь, имеют доступ к производительности, выделенной для этой конкретной очереди. Для каждой очереди ограничения пропускной способности настраиваются администраторами и могут быть как мягкими, так и жесткими.

+ Безопасность (Security). 

Каждая очередь имеет строгие списки ACL, которые контролируют, какие пользователи могут отправлять приложения в отдельные очереди. Кроме того, существуют средства защиты, гарантирующие, что пользователи не смогут просматривать и/или изменять приложения других пользователей. Также поддерживаются роли для каждой очереди и системного администратора.

+ Эластичность (Elasticity). 

Свободные ресурсы могут быть распределены на любые очереди. Когда в будущем от очередей, работающих с пониженной производительностью, возникает потребность в ресурсах, то по мере выполнения запланированных на этих ресурсах задач они назначаются требуемым приложениям (также поддерживается преимущественное право -- preemption). Это гарантирует, что ресурсы доступны для очередей предсказуемо и гибко, тем самым предотвращая искусственное разделение ресурсов в кластере и помогая их использованию.

+ Мультитенантность (Multi-tenancy). 

Предоставляется набор ограничений для предотвращения монополизации ресурсов очереди или кластера одним приложением, пользователем или очередью, чтобы гарантировать, что кластер не перегружен.

+ Работоспособность (Operability):

  + Конфигурация во время выполнения (Runtime Configuration) -- определения и свойства очереди, такие как производительность и списки ACL, могут быть изменены во время выполнения безопасным способом администраторами, минимизируя неудобства для пользователей. Кроме того для пользователей и администраторов предусмотрена консоль, позволяющая просматривать текущее распределение ресурсов по различным очередям в системе. Администраторы могут добавлять дополнительные очереди во время выполнения, но очереди не могут быть удалены во время выполнения, если она не остановлена и имеет ожидающие/запущенные приложения.

  + Дренаж приложений (Drain applications) -- администраторы могут останавливать очереди во время выполнения, чтобы гарантировать, что пока существующие приложения не будут завершены, новые не смогут быть направлены. Если очередь находится в состоянии *STOPPED*, новые приложения не могут быть направлены ни ей самой, ни какой-либо из ее дочерних очередей. Текущие приложения продолжают выполняться и, таким образом, очередь может быть аккуратно дренажирована. Администраторы также могут запускать остановленные очереди.

 

+ Планирование на основе ресурсов (Resource-based Scheduling).

Поддержка ресурсоемких приложений, в которых приложение может опционально определять более высокие требования к ресурсам, чем по умолчанию, тем самым приспосабливая приложения с различными требованиями к ресурсам. В настоящее время память является поддерживаемым требованием к ресурсам.

+ Маппинг очереди на основе пользователя или группы (Queue Mapping based on User or Group).

Функция позволяет пользователям сопоставлять работу с определенной очередью на основе пользователя или группы.

+ Приоритетное планирование (Priority Scheduling). 

Функция позволяет направлять приложения и планировать их с разными приоритетами. Более высокое целочисленное значение указывает на более высокий приоритет для приложения. В настоящее время приоритет приложения поддерживается только для политики упорядочения *FIFO*.

+ Конфигурация абсолютных ресурсов (Absolute Resource Configuration).

Администраторы могут указывать абсолютные ресурсы для очереди вместо предоставления значений в процентах. Это обеспечивает лучший контроль для администраторов в целях настройки необходимого количества ресурсов для конкретной очереди.

+ Динамическое автоматическое создание и управление конечными очередями (Dynamic Auto-Creation and Management of Leaf Queues). 

Функция поддерживает автоматическое создание конечных очередей в сочетании с маппингом очередей, которое в настоящее время поддерживает сопоставления очередей на основе групп пользователей для размещения приложений в очереди. Планировщик также поддерживает управление производительностью для этих очередей на основе политики, настроенной в родительской очереди.


Конфигурация
----------------

Чтобы настроить **ResourceManager** для использования **CapacityScheduler**, необходимо установить в файле *conf/yarn-site.xml* свойство ``yarn.resourcemanager.scheduler.class`` со значением ``org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler``.

*etc/hadoop/capacity-scheduler.xml* -- файл конфигурации для **CapacityScheduler**.

**CapacityScheduler** имеет предопределенную очередь с именем *root*, все очереди в системе являются дочерними по отношению к ней. Очереди можно настроить в ``yarn.scheduler.capacity.root.queues`` со списком дочерних очередей, разделенных запятыми.

Конфигурация для **CapacityScheduler** для настройки иерархии очередей использует концепцию, называемую *путь к очереди* (*queue path*). Путь к очереди -- это полный путь иерархии очереди, начиная с *root*, со знаком точки ``.`` в качестве разделителя.

Дочерние элементы очереди могут быть определены с помощью настройки ``yarn.scheduler.capacity.<queue-path>.queues``. Дочерние очереди при этом не наследуют свойства напрямую от родителя, если не указано иное.

Пример с тремя дочерними очередями верхнего уровня *a*, *b* и *c* и некоторыми подпоследовательностями для *a* и *b*:

::

 <property>
   <name>yarn.scheduler.capacity.root.queues</name>
   <value>a,b,c</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>
 
 <property>
   <name>yarn.scheduler.capacity.root.a.queues</name>
   <value>a1,a2</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>
 
 <property>
   <name>yarn.scheduler.capacity.root.b.queues</name>
   <value>b1,b2,b3</value>
   <description>The queues at the this level (root is the root queue).
   </description>
 </property>


Свойства очереди
^^^^^^^^^^^^^^^^^^

Распределение ресурсов
~~~~~~~~~~~~~~~~~~~~~~~

+ ``yarn.scheduler.capacity.<queue-path>.capacity``

Пропускная способность очереди ИЛИ минимальная пропускная способность очереди абсолютных ресурсов, указывается в процентах в виде числа с плавающей запятой (float, например, *12.5*). Сумма производительности для всех очередей на каждом уровне должна быть равна *100*. Однако, если настроен абсолютный ресурс, сумма абсолютных ресурсов дочерних очередей может быть меньше абсолютной производительности родительского ресурса. Приложения в очереди могут потреблять больше ресурсов, чем пропускная способность очереди, если есть свободные ресурсы, обеспечивающие эластичность.

+ ``yarn.scheduler.capacity.<queue-path>.maximum-capacity``

Максимальная пропускная способность очереди ИЛИ максимальная пропускная способность очереди абсолютных ресурсов, указывается в процентах в виде числа с плавающей запятой (float). Параметр ограничивает эластичность для приложений в очереди: 1) Значение находится в диапазоне от *0* до *100*; 2) Администратор должен убедиться, что абсолютная максимальная производительность больше или равна абсолютной производительности для каждой очереди. Кроме того, установка значения в *-1* задает максимальную производительность в *100%*.

+ ``yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent``

Каждая очередь устанавливает ограничение на процент ресурсов, выделяемых пользователю в любой момент времени при потребности в ресурсах. Пользовательское ограничение может варьироваться между минимальным и максимальным значением. Минимум устанавливает данное свойство, а максимум зависит от количества отправивших приложение пользователей. Например, значение свойства равно *25*. Тогда если два пользователя отправляют приложения в очередь, ни один из них не может использовать более *50%* ресурсов очереди. Если третий пользователь отправляет приложение, то ни один пользователь не может использовать более *33%* ресурсов очереди. При наличии *4 или более* пользователей ни один из них не может использовать более *25%* ресурсов очереди. Значение *100* подразумевает, что ограничения для пользователей не вводятся. По умолчанию устанавливается значение *100*. Значение указывается как целое число (integer).

+ ``yarn.scheduler.capacity.<queue-path>.user-limit-factor``

Множество пропускной способности очереди, которое может быть настроено так, чтобы позволить пользователю получить больше ресурсов. По умолчанию значение равно *1*, что гарантирует, что пользователь никогда не сможет получить больше, чем настроенная производительность очереди, независимо от того, насколько простаивает кластер. Значение указывается как число с плавающей запятой (float).

+ ``yarn.scheduler.capacity.<queue-path>.maximum-allocation-mb``

Максимальный лимит памяти для каждой очереди, выделяемый каждому запросу контейнера в Resource Manager. Параметр переопределяет конфигурацию кластера ``yarn.scheduler.maximum-allocation-mb``. Значение должно быть меньше или равно максимуму кластера.

+ ``yarn.scheduler.capacity.<queue-path>.maximum-allocation-vcores``

Максимальный лимит виртуальных ядер для каждой очереди, выделяемый каждому запросу контейнера в Resource Manager. Параметр переопределяет конфигурацию кластера ``yarn.scheduler.maximum-allocation-vcores``. Значение должно быть меньше или равно максимуму кластера.

+ ``yarn.scheduler.capacity.<queue-path>.user-settings.<user-name>.weight``

Это значение с плавающей запятой (float), которое используется для вычисления предельных значений ресурсов пользователя среди пользователей в очереди. Значение определяет по весу каждого пользователя в большей или меньшей степени, относительно других пользователей в очереди. Например, если пользователь *A* должен получить на *50%* больше ресурсов в очереди, чем пользователи *B* и *C*, это свойство должно быть установлено равным *1,5* для пользователя *A*. При этом для пользователей *B* и *C* должно оставаться значение по умолчанию *1.0*.

Распределение ресурсов с помощью Absolute Resources
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает настройку абсолютных ресурсов вместо предоставления процентной пропускной способности очереди. Как упоминается в конфигурации для ``yarn.scheduler.capacity.<queue-path>.capacity`` и ``yarn.scheduler.capacity.<queue-path>.max-capacity``, администратор может указать значение абсолютного ресурса, например, ``[memory=10240,vcores=12]``. Это допустимая конфигурация, указывающая *10 ГБ* памяти и *12 VCores*.

Ограничения для запущенных и ожидающих приложений
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Для управления запущенными и ожидающими приложениями **CapacityScheduler** поддерживает следующие параметры:

+ ``yarn.scheduler.capacity.maximum-applications / yarn.scheduler.capacity.<queue-path>.maximum-applications``

Максимальное количество приложений в системе, которые могут быть одновременно активными (как запущенными, так и ожидающими). Ограничения в каждой очереди прямо пропорциональны пропускной способности очереди и пользовательским лимитам. Это жесткое ограничение, и любые поданные при его достижении приложения отклоняются. По умолчанию значение равно *10000*. Параметр может быть установлен для всех очередей с помощью ``yarn.scheduler.capacity.maximum-applications``, а также может быть переопределен для каждой очереди путем задания ``yarn.scheduler.capacity.<queue-path>.maximum-applications``. Параметр должен представлять собой целочисленное значение (integer).

+ ``yarn.scheduler.capacity.maximum-am-resource-percent / yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent``

Максимальный процент ресурсов в кластере, которые могут быть использованы для запуска мастера (application masters), контролирующего количество одновременно работающих приложений. Ограничения в каждой очереди прямо пропорциональны пропускной способности очереди и пользовательским лимитам. Указывается как число с плавающей запятой (float), то есть значение *0.5* равно *50%*. По умолчанию задается *10%*. Значение может быть установлено для всех очередей с помощью параметра ``yarn.scheduler.capacity.maximum-am-resource-percent``, а также может быть переопределено для каждой очереди путем задания ``yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent``.

Администрирование и разрешения очереди
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+ ``yarn.scheduler.capacity.<queue-path>.state``

Статус очереди -- *RUNNING* или *STOPPED*. Если очередь находится в состоянии *STOPPED*, новые приложения не могут быть отправлены ни ей самой, ни какой-либо из ее дочерних очередей. Таким образом, если очередь *root* остановлена, никакие приложения не могут быть переданы всему кластеру, текущие приложения продолжают выполняться, и очередь может быть аккуратно дренажирована. Значение указывается в виде именованной константы (enumeration).

+ ``yarn.scheduler.capacity.root.<queue-path>.acl_submit_applications``

Список ACL, который контролирует, кто может подавать приложения в конкретную очередь. Если у пользователя/группы есть необходимые списки управления доступом в очереди или в одной из ее родительских очередей в иерархии, то пользователь/группа может подаваться. Списки ACL для этого свойства наследуются из родительской очереди, если не указано иное.

+ ``yarn.scheduler.capacity.root.<queue-path>.acl_administer_queue``

Список ACL, который контролирует, кто может администрировать приложения в конкретной очереди. Если у пользователя/группы есть необходимые ACL в очереди или в одной из ее родительских очередей в иерархии, то пользователь/группа может администрировать приложения. Списки ACL для этого свойства наследуются из родительской очереди, если не указано иное.

ACL имеет форму *user1,user2 space group1,group2*. Особое значение ``*`` подразумевает *все*. Особое значение ``space`` подразумевает *никто*. Значение по умолчанию ``*`` для очереди *root*, если не указано иное.

Маппинг очереди на основе пользователя или группы
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+ ``yarn.scheduler.capacity.queue-mappings``

Конфигурация определяет маппинг пользователя или группы в определенную очередь. Можно сопоставить одного пользователя или список пользователей с очередями. Синтаксис: ``[u or g]:[name]:[queue_name][,next_mapping]``. Обозначение *u* или *g* указывает, предназначено ли сопоставление для пользователя или группы соответственно; *name* указывает имя пользователя или имя группы. Чтобы указать пользователя, отправившего приложение, можно использовать *%user*. Обозначение *queue_name* указывает имя очереди, для которой должно маппироваться приложение. Чтобы указать имя очереди, совпадающее с именем пользователя, можно использовать *%user*. Чтобы указать имя очереди, совпадающее с именем основной группы, к которой принадлежит пользователь, можно использовать *%primary_group*.

+ ``yarn.scheduler.capacity.queue-mappings-override.enable``

Функция используется для задания возможности переопределения указанных пользователем очередей. Это логическое значение (boolean), и значением по умолчанию является *false*.

Пример:

::

  <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value>u:user1:queue1,g:group1:queue2,u:%user:%user,u:user2:%primary_group</value>
    <description>
      Here, <user1> is mapped to <queue1>, <group1> is mapped to <queue2>, 
      maps users to queues with the same name as user, <user2> is mapped 
      to queue name same as <primary group> respectively. The mappings will be 
      evaluated from left to right, and the first valid mapping will be used.
    </description>
  </property>


Срок приложений в очереди
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+ ``yarn.scheduler.capacity.<queue-path>.maximum-application-lifetime``

Максимальное время жизни отправленного в очередь приложения, задается в секундах. Любое меньшее или равное нулю значение считается как отключенное и является жестким лимитом времени для всех приложений в этой очереди. Если задано положительное значение параметра, любое приложение, отправленное в данную очередь, уничтожается после превышения настроенного срока. Пользователь также может указать срок для каждого приложения в контексте. Срок пользователя переопределяется, если он превышает максимальное время жизни очереди. Это конфигурация на определенный момент времени. Настройка слишком низкого значения приводит к быстрому уничтожению приложения. Функция применима только для leaf-очереди.

+ ``yarn.scheduler.capacity.root.<queue-path>.default-application-lifetime``

Время жизни отправленного в очередь приложения по умолчанию, задается в секундах. Любое меньшее или равное нулю значение считается как отключенное. Если пользователь отправляет приложение с незаданным значением срока, то оно задается автоматически. Это конфигурация на определенный момент времени. Примечание: время жизни по умолчанию не может превышать максимальное время жизни. Функция применима только для leaf-очереди.


Настройка приоритета приложения
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Приоритет приложения работает только совместно с политикой упорядочения по умолчанию *FIFO*. 

Приоритет по умолчанию для приложения может быть на уровне кластера и очереди:

+ Приоритет на уровне кластера -- у любого приложения, отправленного с приоритетом, превышающим приоритет *cluster-max*, происходит сброс приоритета до *cluster-max*. Файл конфигурации для приоритета *cluster-max* -- *$HADOOP_HOME/etc/hadoop/yarn-site.xml*. Параметр ``yarn.cluster.max-application-priority`` определяет максимальный приоритет приложения в кластере.

+ Приоритет на уровне leaf-очереди -- каждой leaf-очереди предоставляется приоритет администратора по умолчанию. Приоритет очереди по умолчанию используется для любого приложения, отправленного без заданного приоритета. Файл конфигурации для приоритета на уровне очереди -- *$HADOOP_HOME/etc/hadoop/capacity-scheduler.xml*. Параметр ``yarn.scheduler.capacity.root.<leaf-queue-path>.default-application-priority`` определяет приоритет приложения по умолчанию в leaf-очереди.

.. important:: Приоритет приложения не изменяется при перемещении приложения в другую очередь


Преимущественное право в Capacity Scheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает возможность преимущественного права (preemption) контейнера от очередей, чье использование ресурсов превышает их гарантированную производительность. Для этого следующие параметры конфигурации должны быть включены в *yarn-site.xml*:

+ ``yarn.resourcemanager.scheduler.monitor.enable``

Включение набора периодического мониторинга (periodic monitors, указанных в ``yarn.resourcemanager.scheduler.monitor.policies``), влияющих на планировщик. Значением по умолчанию является *false*.

+ ``yarn.resourcemanager.scheduler.monitor.policies``

Список классов *SchedulingEditPolicy*, взаимодействующих с планировщиком. Настроенные политики должны быть совместимы с планировщиком. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy*, что совместимо с CapacityScheduler.

Следующие параметры конфигурации могут быть настроены в *yarn-site.xml* для управления преимущественным правом контейнеров, когда класс *ProportionalCapacityPreemptionPolicy* задан для *yarn.resourcemanager.scheduler.monitor.policies*:

+ ``yarn.resourcemanager.monitor.capacity.preemption.observe_only``

Если установлено значение *true*, следует запустить политику, но не влиять на кластер событиями preemption и kill. Значением по умолчанию является *false*.

+ ``yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval``

Время между вызовами политики *ProportionalCapacityPreemptionPolicy* (в миллисекундах). Значение по умолчанию *3000*.

+ ``yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill``

Время между запросом preemption из приложения и уничтожением контейнера (в миллисекундах). Значение по умолчанию *15000*.

+ ``yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round``

Максимальный процент ресурсов для вытеснения по преимущественному праву за один раунд. Управляя этим значением, можно регулировать скорость, с которой контейнеры извлекаются из кластера. После вычисления общего желаемого преимущественного права политика сокращает его в пределах этого лимита. Значение по умолчанию *0.1*.

+ ``yarn.resourcemanager.monitor.capacity.preemption.max_ignored_over_capacity``

Максимальное количество ресурсов, превышающих по преимущественному праву заданную пропускную способность. Параметр определяет мертвую зону вокруг назначенной пропускной способности, которая помогает предотвратить колебания вокруг вычисленного заданного баланса. Высокие значения замедляют производительность и (при отсутствии *natural.completions*) могут препятствовать конвергенции к гарантированной производительности. Значение по умолчанию *0.1*.

+ ``yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor``

Учитывая вычисленное заданное преимущественное право, следует учесть контейнеры с истекающим сроком и выгрузить только этот процент дельты. Параметр определяет скорость геометрической конвергенции в мертвую зону (*MAX_IGNORED_OVER_CAPACITY*). Например, фактор высвобождения (termination factor) *0.5* восстанавливает почти *95%* ресурсов в пределах ``5 * #WAIT_TIME_BEFORE_KILL``, даже при отсутствии естественного завершения (natural termination). Значение по умолчанию составляет *0.2*.

**CapacityScheduler** поддерживает следующие конфигурации в *capacity-scheduler.xml* для управления преимущественным правом контейнеров приложений, отправляемых в очередь:

+ ``yarn.scheduler.capacity.<queue-path>.disable_preemption``

Конфигурацию можно установить в значение *true* для того, чтобы выборочно отключить преимущественное право контейнеров приложений, отправленных в указанную очередь. Свойство применяется только в том случае, если право preemption в масштабе всей системы включено путем настройки ``yarn.resourcemanager.scheduler.monitor.enable`` на *true* и ``yarn.resourcemanager.scheduler.monitor.policies`` на *ProportionalCapacityPreemptionPolicy*. Если данное свойство не установлено, то значение наследуется от родителя очереди. Значением по умолчанию является *false*.

+ ``yarn.scheduler.capacity.<queue-path>.intra-queue-preemption.disable_preemption``

Конфигурация может быть установлена в значение *true* для того, чтобы выборочно отключить внутри очереди преимущественное право контейнеров приложений, отправленных в указанную очередь. Свойство применяется только в том случае, если право preemption в масштабе всей системы включено путем настройки ``yarn.resourcemanager.scheduler.monitor.enable`` в значение *true*, ``yarn.resourcemanager.scheduler.monitor.policies`` на *ProportionalCapacityPreemptionPolicy* и ``yarn.resourcemanager.monitor.capacity.preemption.intra-queue-preemption.enabled`` в значение *true*. Если данное свойство не установлено, то значение наследуется от родителя очереди. Значением по умолчанию является *false*.


Свойства резервирования
~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает параметры для управления созданием, удалением, обновлением и списком резервирований. Важно обратить внимание, что любой пользователь может обновлять, удалять или перечислять свои собственные резервирования. Если списки ACL-резервирования включены, но не определены, доступ будет иметь каждый. В приведенных далее примерах **<queue>* -- это имя очереди. Например, чтобы настроить ACL для управления резервированиями в очереди по умолчанию, следует использовать свойство ``yarn.scheduler.capacity.root.default.acl_administer_reservations``.

+ ``yarn.scheduler.capacity.root.<queue>.acl_administer_reservations``

ACL, который контролирует, кто может управлять резервированием для указанной очереди. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может отправлять, удалять, обновлять и составлять список всех резервирований. ACL для свойства не наследуются.

+ ``yarn.scheduler.capacity.root.<queue>.acl_list_reservations``

ACL, который контролирует, кто может составлять список резервирований для указанной очереди. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может составлять список всех приложений. ACL для свойства не наследуются.

+ ``yarn.scheduler.capacity.root.<queue>.acl_submit_reservations``

ACL, который контролирует, кто может отправлять резервирования в указанную очередь. Если у данного пользователя/группы есть необходимые ACL в этой очереди, то он/она может отправлять резервирование. ACL для свойства не наследуются.


Настройка ReservationSystem с помощью CapacityScheduler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает систему **ReservationSystem**, которая позволяет пользователям резервировать ресурсы заблаговременно. Таким образом приложение может запросить зарезервированные ресурсы во время выполнения, указав *reservationId*. Для этого могут быть настроены следующие параметры конфигурации в *yarn-site.xml*:

+ ``yarn.resourcemanager.reservation-system.enable``

Обязательный параметр: включить **ReservationSystem** в **ResourceManager**. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть **ReservationSystem** не включена.

+ ``yarn.resourcemanager.reservation-system.class``

Необязательный параметр: имя класса **ReservationSystem**. Значение по умолчанию выбирается на основе настроенного планировщика, то есть если настроен **CapacityScheduler**, то классом является *CapacityReservationSystem*.

+ ``yarn.resourcemanager.reservation-system.plan.follower``

Необязательный параметр: имя класса *PlanFollower*, который запускается по таймеру и синхронизирует **CapacityScheduler** с *Plan* и наоборот. Значение по умолчанию выбирается на основе настроенного планировщика, то есть если настроен **CapacityScheduler**, то классом является *CapacitySchedulerPlanFollower*.

+ ``yarn.resourcemanager.reservation-system.planfollower.time-step``

Необязательный параметр: частота таймера *PlanFollower* (в миллисекундах). Значением по умолчанию является *1000*.

**ReservationSystem** интегрирована с иерархией очереди **CapacityScheduler** и может быть настроена для любой *LeafQueue*. Для этого в **CapacityScheduler** поддерживаются следующие параметры:

+ ``yarn.scheduler.capacity.<queue-path>.reservable``

Обязательный параметр: указывает **ReservationSystem**, что ресурсы очереди доступны для резервирования пользователями. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть резервирование в *LeafQueue* не включено.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-agent``

Необязательный параметр: имя класса для использования в целях определения реализации *ReservationAgent*, который принимает попытки разместить запрос пользователя на резервирование в *Plan*. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy*.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-move-on-expiry``

Необязательный параметр, который указывает **ReservationSystem**, следует ли перемещать или уничтожать приложения в родительской резервируемой очереди (настроенной выше) по истечении срока действия соответствующего резервирования. Значение может быть только логическим (boolean), по умолчанию является *true*, означающее, что приложение будет перемещено в резервируемую очередь.

+ ``yarn.scheduler.capacity.<queue-path>.show-reservations-as-queues``

Необязательный параметр для отображения или скрытия очередей резервирования в пользовательском интерфейсе планировщика. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть очереди резервирования скрываются.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-policy``

Необязательный параметр: имя класса для использования в целях определения реализации *SharingPolicy* для проверки новых резервирований на предмет нарушения каких-либо инвариантов. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy*.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-window``

Необязательный параметр, представляющий время в миллисекундах, в течение которого *SharingPolicy* проверяет соблюдение ограничений в *Plan*. Значение по умолчанию составляет один день.

+ ``yarn.scheduler.capacity.<queue-path>.instantaneous-max-capacity``

Необязательный параметр: максимальная пропускная способность в процентах в виде числа с плавающей запятой (float), которую *SharingPolicy* позволяет зарезервировать одному пользователю. Значение по умолчанию равно *1*, то есть *100%*.

+ ``yarn.scheduler.capacity.<queue-path>.average-capacity``

Необязательный параметр: средняя допустимая пропускная способность, агрегируемая в *ReservationWindow* в процентах в виде числа с плавающей запятой (float), которую *SharingPolicy* позволяет зарезервировать одному пользователю. Значение по умолчанию равно *1*, то есть *100%*.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-planner``

Необязательный параметр: имя класса для использования в целях определения реализации *Planner*, вызываемой при падении производительности *Plan* ниже зарезервированных пользователем ресурсов (из-за планового обслуживания или сбоев узла). Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.SimpleCapacityReplanner*, сканирующее *Plan* и жадно удаляющее резервирования в обратном порядке (*LIFO*) до тех пор, пока зарезервированные ресурсы не оказываются в пределах пропускной способности *Plan*.

+ ``yarn.scheduler.capacity.<queue-path>.reservation-enforcement-window``

Необязательный параметр, представляющий время в миллисекундах, в течение которого *Planner* проверяет соблюдение ограничений в *Plan*. Значение по умолчанию составляет один час.


Динамическое автосоздание и управление leaf-очередями
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**CapacityScheduler** поддерживает автоматическое создание наследуемых leaf-очередей, настроенных с включенной данной функцией.

+ Настройка при помощи маппинга

**user-group queue mapping(s)**, перечисленные в ``yarn.scheduler.capacity.queue-mappings``, должны содержать дополнительный параметр очереди, в которую будет осуществляться автосоздание leaf-очередей. Свойства описаны выше в подразделе "Queue Mapping based on User or Group". Так же важно обратить внимание, что в таких родительских очередях необходимо включить автосоздание дочерних очередей, как указано далее.

Пример:

::

 <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value>u:user1:queue1,g:group1:queue2,u:user2:%primary_group,u:%user:parent1.%user</value>
    <description>
      Here, u:%user:parent1.%user mapping allows any <user> other than user1,
      user2 to be mapped to its own user specific leaf queue which
      will be auto-created under <parent1>.
    </description>
  </property>

+ Конфигурация родительской очереди

Функция *Dynamic Queue Auto-Creation and Management* интегрирована с иерархией очереди **CapacityScheduler** и может быть настроена для *ParentQueue* для автоматического создания leaf-очередей. Такие родительские очереди не поддерживают возможность сосуществования автосозданных очередей вместе с другими предварительно сконфигурированными очередями. Свойства:

``yarn.scheduler.capacity.<queue-path>.auto-create-child-queue.enabled`` -- обязательный параметр: указывает для **CapacityScheduler**, что для заданной родительской очереди необходимо включить автосоздание leaf-очереди. Значение может быть только логическим (boolean), по умолчанию является *false*, то есть автосоздание leaf-очереди в *ParentQueue* не включено.

``yarn.scheduler.capacity.<queue-path>.auto-create-child-queue.management-policy`` -- необязательный параметр: имя класса для использования с целью определения реализации *AutoCreatedQueueManagementPolicy*, которая динамически управляет leaf-очередями и их производительностью в данной родительской очереди. Значением по умолчанию является *org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy*. Пользователи или группы могут отправлять приложения в автосозданные leaf-очереди в течение ограниченного времени и прекращать их использование. Следовательно, число leaf-очередей, автосозданных в родительской очереди, может быть больше, чем ее гарантированная пропускная способность. Текущая реализация политики позволяет либо настроить, либо обнулить производительность, исходя из доступности пропускной способности в родительской очереди и порядка отправки приложения через leaf-череди.

+ Настройка при помощи CapacityScheduler

Родительская очередь для автосоздания leaf-очередей поддерживает настройку параметров их шаблона. Автосозданные очереди поддерживают все параметры конфигурации leaf-очереди, за исключением *Queue ACL*, *Absolute Resource*. Списки ACL очереди в настоящее время не настраиваются в шаблоне, но наследуются от родительской очереди. Свойства:

``yarn.scheduler.capacity.<queue-path>.leaf-queue-template.capacity`` -- 

``yarn.scheduler.capacity.<queue-path>.leaf-queue-template.<leaf-queue-property>`` -- 





Changing Queue Configuration
------------------------------




Updating a Container (Experimental - API may change in the future)
--------------------------------------------------------------------
