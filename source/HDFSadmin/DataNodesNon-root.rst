Запуск DataNodes от Non-root
-----------------------------

В данной главе описывается как запускать **DataNodes** от пользователя без прав *root*.



Введение
^^^^^^^^

Исторически сложилось так, что часть конфигурации безопасности для **HDFS** задействовала запуск **DataNode** от пользователя *root* и привязала привилегированные порты для конечных точек сервера. Это было сделано для решения проблемы безопасности, то есть если задание **MapReduce** запущено, а **DataNode** остановился, задачу **MapReduce** можно привязать к порту **DataNode** и потенциально сделать что-то вредоносное. Решением подобных случаев стал запуск **DataNode** от пользователя *root* и использование привилегированных портов. При этом только пользователь *root* может получить доступ к привилегированным портам.

Теперь для безопасного запуска **DataNodes** от пользователя без прав *root* можно использовать **Simple Authentication and Security Layer** (**SASL**). **SASL** используется для обеспечения безопасной связи на уровне протокола.

.. important:: Важно выполнить передачу данных с помощью *root* при запуске DataNodes с использованием SASL для его запуска в конкретной последовательности во всем кластере. В противном случае может возникнуть риск простоя приложения

Для переноса существующего кластера, использующего аутентификацию *root*, с целью использования SASL, сначала необходимо убедиться, что версия **2.6.0** (или более поздняя) развернута для всех узлов кластера, а также для любых внешних приложений, которые необходимо подключить к кластеру. Только версии **2.6.0 +** из HDFS-клиента могут подключаться к **DataNode**, использующему **SASL** для аутентификации протокола передачи данных, поэтому очень важно, чтобы все абоненты имели необходимую версию перед переходом. 

После развертывания версии **2.6.0** (или более поздней) необходимо обновить конфигурацию любых внешних приложений, чтобы включить **SASL**. Если для **SASL** включен клиент **HDFS**, он может успешно подключиться к **DataNode**, работающему с аутентификацией *root* или аутентификацией **SASL**. Изменение конфигурации для всех клиентов гарантирует, что последующие изменения конфигурации в **DataNodes** не нарушат работу приложений. Наконец, каждый отдельный **DataNode** может быть перенесен путем изменения его конфигурации и перезапуска. Допустимо временно сочетать некоторые **DataNodes** с аутентификацией *root* и некоторые **DataNodes**, работающие с аутентификацией **SASL**, в течение периода перехода, поскольку клиент **HDFS**, подключенный для **SASL**, может подключаться к обоим.



Настройка DataNode SASL
^^^^^^^^^^^^^^^^^^^^^^^

Для настройки **DataNode SASL** с безопасным запуском **DataNode** от *non-root* пользователя необходимо выполнить следующие действия:

**1. Выключить DataNode**

Выключить **DataNode** с помощью соответствующих команд.

**2. Включить SASL**

Чтобы включить **DataNode SASL** в файле */etc/hadoop/conf/hdfs-site.xml* настроить свойства.

Свойство *dfs.data.transfer.protection* позволяет использовать **DataNode SASL**. Данному свойству можно установить одно из следующих значений:

+ *authentication* - устанавливает взаимную аутентификацию между клиентом и сервером;
+ *integrity* - в дополнение к аутентификации гарантирует, что man-in-the-middle не может вмешиваться в сообщения, обмен которыми осуществляется между клиентом и сервером;
+ *privacy* - в дополнение к функциям *authentication* и *integrity* он также полностью шифрует сообщения, обмен которыми осуществляется между клиентом и сервером.

Также необходимо установить для свойства *dfs.http.policy* значение *HTTPS_ONLY*. При этом следует указать порты для **DataNode RPC** и HTTP-серверов.

Например:
::

 <property>
    <name>dfs.data.transfer.protection</name>
    <value>integrity</value>
  </property>
 
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:10019</value>
  </property>
 
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:10022</value>
  </property>
 
  <property>
    <name>dfs.http.policy</name>
    <value>HTTPS_ONLY</value>
  </property>

.. important:: Параметр шифрования *dfs.encrypt.data.transfer=true* похож на *dfs.data.transfer.protection=privacy. Эти два параметра являются взаимоисключающими, поэтому они не должны устанавливаться вместе. В случае если оба параметра установлены, *dfs.encrypt.data.transfer* не используется

**3. Обновить настройки экосистемы**

В файле */etc/hadoop/conf/hadoop-env.xml* изменить параметр:
::

 #On secure datanodes, user to run the datanode as after dropping privileges export HADOOP_SECURE_DN_USER=

Строка экспорта *HADOOP_SECURE_DN_USER=hdfs* включает устаревшую конфигурацию безопасности и, чтобы включить **SASL**, должна быть установлена на пустое значение.

**4. Запустить DataNode**

Запустить **DataNode** с помощью соответствующих команд.




























