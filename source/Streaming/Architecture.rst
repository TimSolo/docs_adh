Архитектурные особенности
--------------------------

Обзор
^^^^^^^

Разработанная компанией **Arenadata** платформа **ADS** имеет возможность выступления в качестве единой платформы для обработки всех потоков данных, которые может иметь крупная компания, в реальном времени. С этой целью продуман достаточно широкий набор вариантов использования:

+ **ADS** должна иметь высокую пропускную способность для поддержки потоков событий большого объема, таких как агрегация журнала в режиме реального времени.
+ **ADS** должна грамотно справляться с большими объемами данных, чтобы иметь возможность поддержки периодических загрузок данных из автономных систем.
+ **ADS** должна обрабатывать передачу данных с низкой задержкой с целью обработки большого количества традиционных случаев использования сообщений.

Партицирование и потребительская модель были мотивированы желанием **Arenadata** поддерживать секционированную, распределенную обработку в реальном времени для создания новых унаследованных лент.

Наконец, в тех случаях, когда поток подается в сторонние системы данных для обслуживания, специалисты **Arenadata** понимали, что система должна быть в состоянии гарантировать отказоустойчивость при наличии сбоев в работе машины.

Поддержка перечисленных критериев использования привела **Arenadata** к разработке с несколькими уникальными элементами, более похожими на журнал базы данных, чем на традиционную систему обмена сообщениями. 


Персистентность
^^^^^^^^^^^^^^^^

**ADS** в значительной степени опирается на файловую систему для хранения и кэширования сообщений. Существует общее представление о том, что "диски медленные", и это заставляет людей скептически относиться к тому, что персистентная структура может предложить конкурентоспособную производительность. На самом деле диски и намного медленнее и намного быстрее в зависимости от того, как они используются. И правильно разработанная структура диска часто может быть такой же быстрой, как и сеть.

Ключевым фактом о производительности диска является то, что пропускная способность жестких дисков отличается от латентности диска в течение последнего десятилетия. В результате производительность линейных записей в конфигурации **JBOD** с шестью массивами **SATA RAID-5** размером *7200 об/мин* составляет около *600 МБ/с*, но производительность операции случайной записи составляет всего около *100 к/сек* -- разница более *6000X*. Эти линейные операции чтения и записи являются наиболее предсказуемыми для всех моделей использования и сильно оптимизированы операционной системой. Современная ОС предоставляет технологии опережающего чтения и записи, которые обеспечивают предварительную выборку данных в больших кратных блоках и группируют меньшие логические записи в большие физические записи. Дальнейшее обсуждение этой проблемы можно найти в статье `ACM Queue <https://queue.acm.org/detail.cfm?id=1563874>`_, где показывается, что последовательный доступ к диску может в некоторых случаях быть быстрее, чем случайный доступ к памяти.

Для компенсации расхождения в производительности, современные операционные системы становятся все более агрессивными в использовании основной памяти для кэширования диска. Современная ОС с радостью перенаправит всю свободную память на кэширование диска с небольшим снижением производительности при восстановлении памяти. Все операции чтения и записи на диск будут проходить через этот единый кэш. Данная функция не может быть легко отключена без использования прямого ввода-вывода, поэтому, даже если процесс поддерживает встроенный кэш данных, эти данные, вероятно, будут дублироваться в pagecache операционной системы, эффективно сохраняя все дважды.

Платформа **ADS** строится поверх **JVM**, и каждый, кто проводил какое-либо время с использованием памяти **Java**, знает две вещи:

+ Накладные расходы памяти на объектах очень высоки, что часто удваивает размер хранимых данных;
+ Сбор мусора Java становится все более неудобным и медленным по мере увеличения объема данных in-heap.

В результате этих факторов, используя файловую систему и полагаясь на pagecache, лучше поддерживать кэш in-memory или другую структуру -- **Arenadata**, по крайней мере, удваивает доступный кэш, имея автоматический доступ ко всей свободной памяти и, примерно удваивает повторно, сохраняя компактную структуру байтов, а не отдельные объекты. Это приводит к кэшу до *28-30 ГБ* на машине *32 Гб* без ограничений сборщиков мусора. Кроме того, кэш остается теплым, даже при перезагрузке сервиса. В то время как кэш in-process необходимо  перестраивать в памяти (что для кэша *10 ГБ* может занять до *10 минут*), иначе ему придется стартовать с полностью холодного кэша (что означает ужасную начальную производительность). К тому же значительно упрощается код, поскольку вся логика поддержания согласованности между кэшем и файловой системой теперь находится в ОС, которая имеет тенденцию делать это более эффективно и правильно, чем однократные попытки in-process. Если использование диска способствует линейному чтению, то опережающее чтение эффективно заполняет этот кэш полезными данными на каждом диске.

Предлагается очень простая структура: вместо того, чтобы содержать как можно больше данных in-memory, и потом в панике полностью очищать файловую систему, когда заканчивается пространство, мы инвертируем это -- все данные сразу же записываются в постоянный журнал файловой системы без необходимости сброса на диск. По сути это просто означает, что данные переносятся в pagecache ядра.

Такой ориентированный на pagecache стиль описывается в статье `Notes from the Architect <http://varnish-cache.org/docs/trunk/phk/notes.html>`_.

Персистентная структура данных, используемая в системах обмена сообщениями, часто представляет собой очередь для каждого потребителя со связанным BTree или другими структурами данных произвольного доступа общего назначения для поддержки метаданных о сообщениях. BTrees являются наиболее универсальной структурой данных и позволяют поддерживать широкий спектр транзакционной и нетранзакционной семантики в системе обмена сообщениями. Они имеют довольно высокую стоимость, однако: Btree операции -- *O* (*log N*). Обычно *O* (*log N*) считается эквивалентным постоянному времени, но это не относится к дисковым операциям. Диск выполняет поиск за *10 мс*, и каждый диск может делать только один поиск за раз, поэтому параллелизм ограничен. Следовательно, даже небольшое количество запросов на диск приводит к очень высоким накладным расходам. Поскольку системы хранения данных сочетают очень быстрые кэшированные операции с очень медленными физическими дисковыми операциями, наблюдаемая производительность древовидных структур часто суперлинейна по мере увеличения данных с фиксированным кэшем, то есть дублирование данных -- это намного хуже по сравнению с вдвое медленной скоростью.

Интуитивно персистентная очередь может быть построена на простых операциях чтения и добавления к файлам, как это обычно бывает с решениями по ведению журнала. Такая структура имеет преимущество в том, что все операции -- *O* (*1*), и операции чтения не блокируют операции записи или друг друга. Это имеет очевидные преимущества в производительности, так как она полностью отделена от размера данных. Один сервер теперь может в полной мере использовать ряд дешевых и низкоскоростных дисков *1+TB SATA*. Хотя у них низкая производительность поиска, она приемлема для многочисленных операций чтения и записи и достигает 1/3 цены и 3x емкости.

Имея доступ к практически неограниченному дисковому пространству без какого-либо снижения производительности **Arenadata** может предоставить некоторые функции, которые обычно не встречаются в системах обмена сообщениями. Например, в **ADS** вместо того, чтобы удалять сообщения сразу после их считывания, можно сохранять их в течение относительно длительного периода. Это приводит к большой гибкости для потребителей.


Производительность
^^^^^^^^^^^^^^^^^^^^^

Специалисты **Arenadata** приложили значительные усилия для повышения производительности платформы. Одним из основных вариантов использования является обработка данных веб-активности, которая очень велика: каждый просмотр страницы может генерировать десятки записей. Кроме того, каждое опубликованное сообщение читается как минимум одним потребителем (а часто -- многими), поэтому есть стремление сделать потребление как можно более дешевым.

По опыту разработки и эксплуатации ряда аналогичных систем выяснилось, что производительность является ключом к эффективным многопользовательским операциям. Если нижестоящий сервис инфраструктуры может легко стать узким местом из-за незначительного увеличения по использованию приложения, то такие небольшие изменения часто создают проблемы. В результате приложение выйдет из строя под нагрузкой перед инфраструктурой. Это особенно важно при попытке запуска централизованного сервиса, поддерживающего десятки или сотни приложений в централизованном кластере, поскольку изменения в шаблонах использования происходят почти ежедневно.

После устранения неудачных шаблонов доступа к диску остается две общие причины неэффективности в подобном типе системы: слишком много мелких операций ввода-вывода и избыточное копирование байтов.

Проблема множества мелких операций ввода-вывода происходит как между клиентом и сервером, так и в собственных персистентных операциях сервера. Во избежание этого протокол **Arenadata** построен вокруг абстракции "набор сообщений", группирующей сообщения. Это позволяет сетевым запросам объединять сообщения вместе, амортизировав накладные расходы в сети, а не отправлять одно сообщение за раз. Сервер единоразово добавляет фрагменты сообщений в свой журнал, а потребитель извлекает большие линейные фрагменты за раз.

Такая простая оптимизация на порядок увеличивает скорость работы. Пакетирование приводит к увеличению сетевых пакетов, последовательных операций с дисками и смежными блоками памяти и т.д., что позволяет платформе **ADS** превращать поток случайных сообщений в линейные записи, которые поступают потребителям.

Другая непродуктивность заключается в копировании байтов. При низких скоростях передачи сообщений это не проблема, но под нагрузкой влияние значительно. Во избежание этого **ADS** использует стандартный бинарный формат сообщений, который совместно применяется поставщиком, брокером и потребителем (таким образом, блоки данных могут передаваться без изменений).

Журнал сообщений, поддерживаемый брокером, сам по себе является просто каталогом файлов, каждый из которых заполняется рядом наборов сообщений, которые были записаны на диск в том же формате, который используется поставщиком и потребителем. Поддержка общего формата позволяет оптимизировать наиболее важную операцию -- сетевую передачу персистентных блоков журнала. Современные операционные системы unix предлагают высоко оптимизированный путь кода для передачи данных из pagecache в сокет; в Linux это делается с помощью системного вызова sendfile.

Путь передачи данных из файла в сокет заключается в следующем:

1. Операционная система считывает данные с диска в pagecache в пространстве ядра.
2. Приложение считывает данные из пространства ядра в буфер пространства пользователя.
3. Приложение записывает данные в пространство ядра в буфер сокета.
4. Операционная система копирует данные из буфера сокета в буфер сетевого адаптера по сети.

Такой метод явно неэффективен -- он предполагает четыре операции копирования и два системных вызова. А при использовании sendfile повторное копирование (zero-copy) исключается, позволяя ОС напрямую отправлять данные из pagecache в сеть. Таким образом, из оптимизированного пути требуется только последняя копия в буфер сетевого адаптера.

Как правило, общий вариант использования состоит из нескольких потребителей топика. При приведенной выше оптимизации данные копируются в pagecache только один раз и при каждом потреблении используются повторно, а не хранятся в памяти и копируются в пространство пользователя при каждом чтении. Это позволяет считывать сообщения со скоростью, приближенной к пределу сетевого подключения.

Такая комбинация pagecache и sendfile означает, что в кластере **ADS** нет активности чтения на дисках, поскольку данные полностью обслуживаются из кэша.

Дополнительные сведения о поддержке sendfile и zero-copy в **Java** приведены в `статье <http://www.ibm.com/developerworks/linux/library/j-zerocopy>`_.



Поставщики данных
^^^^^^^^^^^^^^^^^^


Потребители данных
^^^^^^^^^^^^^^^^^^


Механизм доставки сообщений
^^^^^^^^^^^^^^^^^^^^^^^^^^^


Репликация
^^^^^^^^^^^


Сжатие журналов
^^^^^^^^^^^^^^^^
